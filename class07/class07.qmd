---
title: "Class 07: Machine Learning 1"
author: "Katelyn Wei (A16682595)"
format: pdf
---

# Clustering

We will start today's lab with clustering methods, in particular so-called K-means. The main function for this in R is `kmeans()`.

Let's try it on some made up data where we know what the answer should be.

```{r}
x <- rnorm(10000, mean = 3)
hist(x)
```

60 points
```{r}
tmp <- c(rnorm(30, mean = 3), rnorm(30, -3))
x <- cbind(x = tmp, y = rev(tmp))
head(x)
```

We can pass this to the base R `plot()` function for a quick look.
```{r}
plot(x)
```

```{r}
k <- kmeans(x, centers = 2, nstart = 20)
k
```

> Q1. How many points are in each cluster?

```{r}
k$size
```

> Q2. Cluster membership?

```{r}
k$cluster
```

> Q3. Cluster centers?

```{r}
k$centers
```

> Q4. Plot my clustering results

```{r}
plot(x, col = k$cluster, pch = 16)
```

> Q5. Cluster the data again with kmeans() into 4 groups and plot the results.

```{r}
k4 <- kmeans(x, centers = 4, nstart = 20)
plot(x, col = k4$cluster, pch = 16)
```

Kmeans is very popular mostly because it's fast and relatively straightforward to run and understand. It has a big limitation in that you need to tell it how many groups (k, or centers) you want.



# Hierarchical Clustering

The main function is `hclust()`. You have to pass it in a "distance matrix", not just your input data.

You can generate a distance matrix with the `dist()` function.

```{r}
hc <- hclust(dist(x))
hc
```

```{r}
plot(hc)
```

To find clusters (cluster membership vector) from a `hclust()` result we can "cut" the tree at a certain height that we like. `abline()` adds a line to a plot and `cutree()` cuts 

```{r}
plot(hc)
abline(h=8, col = "red")
grps <- cutree(hc, h=8)
```

```{r}
table(grps)
```

> Q6. Plot the hclust results

```{r}
plot(x, col = grps)
```


# Lab 7

First we will read the provided UK_foods.csv input file:
```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url)
x
```

> Q1. How many rows and columns are in your new data frame named x? What R functions could you use to answer this questions?

```{r}
dim(x)
```

Checking the data:
```{r}
## Preview the first 6 rows
head(x)
```

The row names were not set properly. One way to fix this is to set `rownames()` to the 1st column, then remove the 1st column with `-1`. THIS IS DANGEROUS. Re-running the code would delete data.
```{r}
rownames(x) <- x[,1]
x <- x[,-1]
head(x)
```

Instead, we can re-read the data and use the row.names argument to set it to the 1st column:

```{r}
x <- read.csv(url, row.names=1)
head(x)
```

> Q2. Which approach to solving the ‘row-names problem’ mentioned above do you prefer and why? Is one approach more robust than another under certain circumstances?

The second one because it prevents an accidental error from happening.

```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```

> Q3: Changing what optional argument in the above barplot() function results in the following plot?

```{r}
# Setting beside to False
barplot(as.matrix(x), beside=F, col=rainbow(nrow(x)))
```


> Q5: Generating all pairwise plots may help somewhat. Can you make sense of the following code and resulting figure? What does it mean if a given point lies on the diagonal for a given plot?

The `pairs()` function makes a matrix of scatterplots that compare pairs of categories at a time(countries in this case). If a point lies on the diagonal, it means that variable doesn't differ much between the two countries.
```{r}
pairs(x, col=rainbow(10), pch=16)
```


> Q6. What is the main differences between N. Ireland and the other countries of the UK in terms of this data-set?

The main differences are fresh potatoes, cheese and alcohol consumption.



## Principal Component Analysis (PCA)

PCA can help us make sense of these types of datasets. Let's see how it works.

The main function in "base" R is called `prcomp()`. In this case we want to first transpose our input `x` so the columns are the food types and the rows are the countries.

```{r}
head( t(x) )
```

```{r}
pca <- prcomp( t(x) )
summary(pca)
```
> Q7. Complete the code below to generate a plot of PC1 vs PC2. The second line adds text labels over the data points.

```{r}
# plotting PC1 compared to PC2
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500))
text(pca$x[,1], pca$x[,2], colnames(x))
```

> Q8. Customize your plot so that the colors of the country names match the colors in our UK and Ireland map and table at start of this document.

```{r}
# adding color
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500))
text(pca$x[,1], pca$x[,2], colnames(x), col=c("orange", "red", "blue", "darkgreen"))
```



The "loading scores" in this plot tell us how much the original variables (in our case the foods) contribute to the new variables i.e. the PCs:

```{r}
head(pca$rotation)

## Let's focus on PC1 as it accounts for > 90% of variance
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,1], las=2 )
```

> Q9: Generate a similar ‘loadings plot’ for PC2. What two food groups feature prominantely and what does PC2 maninly tell us about?

Fresh potatoes and soft drinks are the most prominent. PC2 mainly tells us that fresh potatoes and soft drinks are what's driving the differences between the countries.
```{r}
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,2], las=2 )
```

